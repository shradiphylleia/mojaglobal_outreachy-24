Update:-rejected :(
# mojaglobal_outreachy-24
Land Sector Data Analysis-Developing a FLINT Forest Monitoring tool using Land Sector datasets


  # Past experience with this community:
I was not aware about the project before the Outreachy application. However, during the contribution period, I actively tried to involve myself in the community and tried to hone my skills to help contribute to the project. I joined the Slack community (@communiCAtion) and interacted with the other contributors whenever I had any doubt. The Flint handbook gave me an overview of the project and its possible use case. This helped me gain perspective which was necessary to get started.
I sharpened my database skills and started learning advanced concepts of MongoDB from the official blog posts on their channel.
I learnt how to work with Json files and manipulate them while working on the tasks given during the contribution period.
I worked with a dataset this large for the first time and used the help of my fellow contributors to get a hold of the large dataset. I learned how to process geospatial data using GeoPandas and utilized my skills in pandas and Matplotlib to my advantage in order to better formulate better conclusions.
I also learned how to ask for feedback and make a conclusive report to showcase to stakeholders at various levels as put to use in the analysis for Olympic National Park. ( https://github.com/shradiphylleia/mojaglobal_outreachy-24/blob/main/Outreachy_2024/Week2/OLYMPIC%20NATIONAL%20PARK.docx )

While working I realized there is lack of resources for the tool and if allowed the opportunity, I would try to make the documentation more coherent, provide documentation for dataset, create tutorial on how to download the tool and improve the file structure so that the difficulties I faced do not persist for future users.


  # Past experience with other communities:
I have been using Talend Open Studio. The software enables users to build basic pipelines for ETL and data integration tasks. Unfortunately, due to poor adoption and lack of contribution the open-source product retired in January of 2024.This made me motivated to find more ways to contribute to open-source products which would benefit from my technical and non-technical skills so they can cater to users worldwide.

I am actively using MySQL database to implement database related projects. The community support and extensive workflow maintained seamlessly online is astonishing and helped me understand how to work in the community ask questions to the maintainers.

I have contributed to various open-source projects. I am a student developer at the Open-Source Community based out of my university. During the summer I participated in their Open-Source Summer Program and was later selected for as a student developer. This has enabled me to get comfortable with GitHub. I learned to ask for help. Understand codebase and workflow. Develop understanding of other developers coding preferences and style in-order to contribute efficiently to collaborative.

OSoC-Sensitive-Information-Blurring-App
Project Repo: https://github.com/upes-open/OSoC-Sensitive-Information-Blurring-App
Contribution repo: https://github.com/shradiphylleia/OSoC-Sensitive-Information-Blurring-App
Worked to develop high-fidelity wireframe and prototype the high-fidelity wireframe using html and CSS
Worked to develop the dataset that was used to test and train the model on.

OSoC-bio.lnk
Project repo: https://github.com/upes-open/OSoC-bio.lnk
Contribution repo: https://github.com/shradiphylleia/OSoC-bio.lnk
Worked to develop high-fidelity wireframe.

OSoC-Resource-Sharing-App
Project repo:  https://github.com/upes-open/OSoC-Resource-Sharing-App
Contribution: https://github.com/upes-open/OSoC-Resource-Sharing-App/pulls/shradiphylleia
Worked to implement error pages using html and css.

OSoC-Swayam-Bazaar
Project repo: https://github.com/upes-open/OSoC-Swayam-Bazaar
Contribution repo: https://github.com/shradiphylleia/OSoC-Swayam-Bazaar
Worked on the frontend using html, css and frontend frameworks.

  # Relevant Projects:
Link: https://github.com/shradiphylleia/mojaglobal_outreachy-24
The tasks given during the contribution period helped me gain understanding of working with large datasets and gave me the opportunity to learn GeoPandas. I went through the dataset and tried to understand the geographic terminologies in order to get a head start. I used pandas to get a hold of the data in more concise and coherent way using dataframe to visualize the data in tabular format. I also used pie-charts to understand how the data is spread out. For the week 2 task I had to learn how to hunt for external data that was relevant to the problem statement. I learnt how to create a report from the data which could serve as infographic to no-technical audience. 
The geo-files had different CRS (Co-ordinate Reference system) which need to be transformed in order to visualize the intersection points.

Link: https://github.com/shradiphylleia/PYTHON_learning
Prior to this I worked on my python skills solving various fundamental to intermediate programming questions to learn the fundamentals.

Link: https://www.kaggle.com/code/diphylleia/s4e1-sub1
By participating in competitions and working on versatile datasets I have developed adaptability to understand complex datasets and apply machine learning algorithm. I worked on preprocessing and cleaning the data. Addressed the imbalance by using SMOTE. Also used one-hot encoding technique on categorical data. This project showcases my ability to perform operations on the given data using matplotlib, pandas and NumPy. I can build pipelines using sikitlearn to perform data cleaning and preprocessing task and build data pipelines.

Link: https://github.com/shradiphylleia/probability-statistics-ClassOf2026
I worked on the implementation of statistical test from scratch and minimum usage of libraries using Python. This project showcases my problem-solving approach by breaking down the tasks into sub-problems to achieve the end goal. While working on this project I had to fine-tune my statistical test to perform on given test-set which helped me learn how to test my code and fine tune it to a given use case.

I am learning relational database management by implementing a project on Online Food delivery system. I am using MySQL workbench to accomplish this task. The project has since helped me how to query database using commands, how to define the database and insert data into the tables. How to establish relationships between various tables in the database and manipulate the stored data. 

I have also worked on data analysis projects using Tableau. I believe experience with other data analysis tools can help me contribute to creating one which serves to the use case of FLINT.


  # Outreachy internship project timeline:
All operations and processes will be documented from the beginning to avoid communication errors.

WEEK 1: Understanding problem statement and collecting the required resources.
->Gather necessary papers and documentation.

WEEK 2: Setup and Environment configuration.  

->Setting up the tools to perform operations.
->Install necessary dependencies and platform tools.
->Collect additional data resources (if required)

WEEK 3: Study the Land Sector datasets and prepare workflow for identification of small squares of forests.

->Read research papers and understand the algorithms associated to the problem statement.
->Ask for feedback on the workflow and understanding of the problem statement.
->Revise the plan (if required)
->Fix any issues (if there in environment setting)

WEEK 4: Start development on the GCBM configuration tool.

WEEK 5: Implementation of feedback on the configuration tool.
Start preparation for simulation dispatch using FLINT.CLOUD templates.
++Checkpoint: Have basic logic/algorithm at hand.

WEEK 6-WEEK 8: Development of configuration tool and simulation work.
++Checkpoint week 7: Basic version of the GCBM configuration tool that can generate configurations for a small subset of forest squares.

WEEK 9-WEEK 11: Integration of CI/CD based workflow to store the results to a remote storage platform.

WEEK 12: Testing, quality check, integrating feedback.

WEEK 13: Testing, quality check and setting up the final documentation.
